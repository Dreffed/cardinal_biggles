# Global LLM Configuration
llm:
  # Default provider for all agents (can be overridden per agent)
  default_provider: "ollama"
  default_model: "llama3.1"

  # Provider-specific settings
  providers:
    ollama:
      base_url: "http://localhost:11434"
      models:
        fast: "llama3.1:8b"      # Fast, cheap tasks
        standard: "llama3.1"      # Standard tasks
        powerful: "llama3.1:70b"  # Complex tasks
      default_temperature: 0.1
      timeout: 120

    openai:
      api_key: "${OPENAI_API_KEY}"  # From environment variable
      models:
        fast: "gpt-3.5-turbo"
        standard: "gpt-4"
        powerful: "gpt-4-turbo"
      default_temperature: 0.1
      max_tokens: 4000
      timeout: 60

    claude:
      api_key: "${ANTHROPIC_API_KEY}"
      models:
        fast: "claude-3-haiku-20240307"
        standard: "claude-3-sonnet-20240229"
        powerful: "claude-3-opus-20240229"
      default_temperature: 0.1
      max_tokens: 4000
      timeout: 60

    perplexity:
      api_key: "${PERPLEXITY_API_KEY}"
      base_url: "https://api.perplexity.ai"
      models:
        fast: "llama-3.1-sonar-small-128k-online"
        standard: "llama-3.1-sonar-large-128k-online"
        powerful: "llama-3.1-sonar-huge-128k-online"
      default_temperature: 0.2
      max_tokens: 4000
      search_recency_filter: "month"  # Perplexity-specific: month, week, day
      timeout: 90

# Agent-Specific LLM Configuration
agents:
  coordinator:
    provider: "ollama"  # Coordinator doesn't need expensive models
    model: "llama3.1"
    temperature: 0.1

  trend_scout:
    provider: "perplexity"  # Perplexity has built-in web search!
    model: "llama-3.1-sonar-large-128k-online"
    temperature: 0.2
    # Perplexity-specific settings
    search_domain_filter: null  # null = all domains
    return_citations: true
    return_images: false

  historian:
    provider: "perplexity"  # Good for historical research with web search
    model: "llama-3.1-sonar-large-128k-online"
    temperature: 0.1
    search_recency_filter: null  # Search all time periods

  scholar:
    provider: "claude"  # Claude excels at academic analysis
    model: "claude-3-sonnet-20240229"
    temperature: 0.1
    # Could use GPT-4 as alternative
    fallback_provider: "openai"
    fallback_model: "gpt-4"

  journalist:
    provider: "perplexity"  # Best for current news with web search
    model: "llama-3.1-sonar-large-128k-online"
    temperature: 0.2
    search_recency_filter: "month"  # Recent news only

  bibliophile:
    provider: "claude"  # Good at long-form content analysis
    model: "claude-3-sonnet-20240229"
    temperature: 0.1

  reporter:
    provider: "claude"  # Best at synthesis and writing
    model: "claude-3-opus-20240229"  # Use most powerful for final report
    temperature: 0.2
    # Fallback to GPT-4 if Claude quota exceeded
    fallback_provider: "openai"
    fallback_model: "gpt-4-turbo"

# Knowledge Store Configuration
knowledge_store:
  type: "chromadb"  # chromadb, qdrant, simple
  persist_directory: "./data/knowledge_store"
  collection_name: "research_knowledge"
  embedding_model: "all-MiniLM-L6-v2"

# Research Configuration
research:
  # Default research parameters
  trend_scout:
    max_trends: 5
    timeframe: "2024-2025"

  historian:
    depth: "comprehensive"  # comprehensive, standard, summary
    min_sources: 5

  scholar:
    min_papers: 5
    max_papers: 15
    recency_years: 3

  journalist:
    min_articles: 10
    max_articles: 25
    days_back: 90

  bibliophile:
    min_books: 5
    max_books: 10

# Output Configuration
output:
  default_format: "markdown"
  include_metadata: true
  include_reference_tables: true
  include_url_validation: true
  save_intermediate_results: true
  output_directory: "./reports"

# Logging and Monitoring
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR
  file: "./logs/orchestrator.log"
  console: true
  include_timestamps: true
  track_costs: true  # Track API costs

# API Server (future)
api:
  enabled: false
  host: "0.0.0.0"
  port: 8000
  cors_origins: ["*"]
